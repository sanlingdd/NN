每次大涨都卖一点，每次大跌都买一点

1. Decision Tree
	ID3
	Find the best feature that can split the data into correct subset. : The lowest entropy is just the feature. Iterate the process until all the features is handled.
	throw away some sub branch which have little influnce on the performance, can solve overfitting problem
	all features should be discrete set. continus features should be split into serveral region due to domain knowledge.
	
	
	C4.5
	Use the information entropy change rate, instead of entropy.
	cutting of some of the branches, can handle overfitting
	can handle continous data
	can handle un-complete data
	

2. KNN
   Classifier that based the NODE which is around. To identify which class it belongs to, just look at the K nearest nodes, if most of the nodes belongs to set A, then, the node belong to A
   NNN: do mean normally, but if we give each node a weight, then the nearby nodes have higher weight, those far node nearly NO weights.
   
   Locally weight reggestion: something like bagging, use nearby NODES to do reggestion and combine the results.
   
   distance:Euclidean Distance, Manhaton Distance, similaririty

3. Ensembling learning
   Use some simple hypothysis to do reggestion or classification and then combine the results. Also using reggestion or classification. CNN also can use this kind of method.
      
   
4. Bagging
   Bagging: randomly choose a subset and do reggestion, then randomly choose another subset then do reggestion. You can K times reggestion and combine all the hypothysis together.
   combine can use mean, while can use another reggestion.
   
5. BOOSTING
   BOOSTING do not treat all training example equally, those examples predict wrongly will have high weight in the next epoch training until the hypothysis have good performance.
   Kind of combine algorithm.
   
   AdaBOOSTING
   equally give all the training examples weight the same weight, and get hypthesis G1
   get the predict error training examples, and give higher weight to them and get G2
   G3, G4
   sign(Sum(w1*G1,w2*G2,w3*G3,...,wn*Gn)) : binary classification
   
   ==> normalization the result ==> sgn( sum(g1,g2,..,gn) / sum(w1,w2,...,w2))  the result tend to fall in -1 and 1, those near 1 is true, near -1 is false.
   
   ==> confidence right high: near 1, confidence low right, nearly 0 but great than 0. ==> more and hypothysis will enlarge the margin between the -1 and 1 
   
   ==> g1, g2,...,gn are weaker learners, those at least do better than possibility. Strong learner is those do much better than possibility.
   
   ==> pink noise: uniform nose
	   white noise: Gaussian noise

7. SVM: kernel, Gaos kernel, Normal kernel
	Seperate the data, and the hyperplanes as far as well to the training examples. That is to ensure the margin max size.
	User kernel to project low higher dimensional space, then that might be lenear seperatable.
	kernel is something about distance and similaririty. 

8. Learning Theory
   How many training example is enough ? for PAC learnable this set.
   Give hypothysis set H, 
   We want the error rate to eposilon e < 0.1, and the failure rate(possibility) no more than delta d < 0.2 
   For binary classification, like decision tree, we have finite hypothysis.
   m >= 1 / e *( ln |H| + ln(1 / d))     |H| is the size of hypothysis space
   
   For infinite hypothysis,
   
   VC Dimension is the number of parameters + 1
   
   m >= 1/ e *( 8 * VC(H) * log2 13 / e + 4 * log2 2/ d ) 

   
9. Bayes Learning
   
   Give a data Set D and a hypothysis set H, we are going to find the hypothysis under the given Data, that is
   argmax( P(H|D) ) ==> argmax ( P (D|H) * P(H) / P(D) )  equal to === >  argmax( P(D | H) * P(H) ) as P(D) is 1, we have already got the data.
                    ==> argmax(P(D|H)) as P(H) is a scalar with our domain knowledge
					==> argmax P(D|H) that is maximum likehood Estimation MLE
	
	asume D, is generated by a hypothysis: f(x) + eposilon, and eposilon is the error and it has Gaussian Distribution, then 
	argmax P(D|H) ==> argmax II Pi(Di|H) ==> argmax II 1 / 根号2 *pi * delta * e^ (-1/2(Hi - Di)^2 / delta^2)
	                                     ==> log ( argmax II 1 / 根号2 *pi * delta * e^ (-1/2(Hi - Di)^2 / delta^2) )
										 ==> argmax(- (di - hi)^2 / delta^2 ) 
										 ==> argmax(-(di - hi)^2)
										 ==> argmin((di - hi)^2) this L2L Loss, square loss
										 
    Also argmax P(H|D)  ===> argmax P(D | H) * P(H) ==> argmax lg P(D|H) + lg P(H)
	                    ===> margmin -lg P(D|H) - lg P(H) This is cross entropy loss
					
   
10. Bayes Network(features join distribution)： Posiblity net in typological order,
	Bayes Reference ~ Approximate reference : 
	近似推理, three rules:
	Marginalization: P(x) = sum(P(x,y)) = P(x|y) + P(Not x | y)
    Chain Rule: P(x,y) = P(X) * P(x|y)
	Bayes Rule: P(x|y) = P(y|x) * P(x) / P(y)
    
	Naive Bayes Classifier: suitable in descrete set of features for binary classification like decision tree, not suitable for continous features
	Given P = 1, spam, collect features like words: Sell(true/false), Money(true/fase),Make(true/false)
	P(S| features1, feature2,feature3,...) = P(feature1,feature2,..,featurn|S) * P(S) / nomailization number ==>features are independent
	                                       = P(feature1)*P(feature2)*...*P(featuren) * P(S) / nomailization number
	
	Reference is cheap
	Feature features, finite, linear parameters
	If the data is large enough, naive bayes is good enough
	1. not easy to ensure feature are contitional independent, while if you made the right classification, the posibility little error is tolentable
	2. For some features that is rare to come up, it is easy to have 0 posibility, then even other features is high, the whole result is 0
		smooth the possibility to ensure no 0 possibility
   
11. Randomized Optimization
	optimization problem
	- Rout finding
	- Root finding
	- NN
	- All Machine Learning
	
	Simulated Annealing(模拟退火): Not always downside direction when gradient desending but also has a possibility to increase that 
	P = exp( - deviative / T ), T is a control parameter T > 0, P = exp(f(x') - f(x) / T)
	==> T -> 0, it is gradient desending
	==> T -> infinite large, it is random walk. 
	decrease T slowly
	
	Genetic algorithm(遗传算法)：NO deviative, NO need function to be continous
	遗传算法的基本运算过程如下：
	a)初始化：设置进化代数计数器t=0，设置最大进化代数T，随机生成M个个体作为初始群体P(0)。
	b)个体评价：计算群体P(t)中各个个体的适应度。
	c)选择运算:将选择算子作用于群体。选择的目的是把优化的个体直接遗传到下一代或通过配对交叉产生新的个体再遗传到下一代。选择操作是建立在群体中个体的适应度评估基础上的。
	d)交叉(crossover)运算：将交叉算子作用于群体。遗传算法中起核心作用的就是交叉算子。
	e)变异运算：将变异算子作用于群体。即是对群体中的个体串的某些基因座上的基因值作变动。
	群体P(t)经过选择、交叉、变异运算之后得到下一代群体P(t+1)。
	f)终止条件判断:若t=T,则以进化过程中所得到的具有最大适应度个体作为最优解输出，终止计算。
	
	P0 = initial size of population K
	repeat:
	compute fitness of all X
	select most fit individuals
	Pair up individuals(crossover,mutation), replace those less fit individuals
	
	One Point Crossover: 1100 0011 // 0000 1111 ==> 1,4 & 2 3 => 1100 1111, 0000 0011
	Uniform crossover: randomize at each bit of position 
	
------	Unsupervised learning -----
12. Single Linkage Clusterring: Nearest two points or mean distance, Center Points distance
	find the closest two points and merge them together, cluster is treated as big point, until K cluster remaining.
	
	Cluster Distance
	1、single link(nearest neign clusterring)
	2、complete link(farthest neighbour clustering.)
	3、group average: mean distance
	4、Ward方法: Squre Loss Increase Least Cluster
	5、质心方法: cluster center distance
	6、Lance-Williams公式
	
	SMC
	
	K-Means: picker K points randomly, and calculate the lefte points distance to them, choose the nearest one and then make the cluster.
	then recalculate the AVG center points of the K Cluster and recalculate the left points. When the center doesn't change,the clusterring is done.
	
	DBSCAN: Density Based Spatial Clusterring of Applications With Noise
	Two Parameter:
	Density Scope: eps
	Minimal Points: minPointNumber
	Choose A point and draw a circle, if the number of points in the circle is greater than minPointNumber, then, make it a cluster, find another point on the edge and draw 
	a circle, if the points in the circle is more than minPointNumber, then merge the points, repeat until all the points visited. Those not in a cluster are noise.
	
	Canopy: Used to chose K and Center Point for K-Means
	T1, T2: T1 > T2
	while D is not empty
	  select element d from D to initialize canopy c
      remove d from D
      Loop through remaining elements in D
           if distance between d_i and c < T1 : add element to the canopy c
           if distance between d_i and c < T2 : remove element from D
      end
      add canopy c to the list of canopies C
	end
	
	Binary K Means: set K=2, and in the sub cluster, use K-Means again with K=2
	
	Expection Maximum Algorithm: EM Gaussian Joint Distribution.
	firstly randomly choose 2 points. As center(then the center is the Gaussian E),  (E of Gaussian is the mean of the cluster)
	Calculate all the points possibility belongs to the Cluster with Gaussian distribution P = e^(- 1/2 * delt^2 * (xi - E)^2), choose the highest posibility cluster and recalculate the E(The center, the mean of all cluster points)
	iterate util coverage. Converge when the border doesn't move, we can set border to p between 0.45 to 0.55.
	EM: Maximum likehood estimate 
	Don't know A and B, but given A, will know B, backward is the same.
	Then, randomly get A, estimat B, then with B, calculate A. A->B, B-A, until the function converge.
	
	Property of EM
	1. no decreasing likehood, every iteraction, the likehood is always improve.
	2. Will not converge sometimes(very little, like K means bad initialization)
	3. Can not diverge
	4. Sometime get local optima - Randomly restart.
	5. Work with any distribution
	
	
	SAS, SPSS, R
	
13. Feature Selection
	Decision Tree: the earlier feature used in decision tree are more important than the following as we use the entropy or entropy change rate to select feature.
	
	Relevance: 
	xi feature is strongly relevant if remove xi will seriouly degrade B.O.C. Bayes Optimal Classifier
	xi is weakly relevant if remove xi doesn't hurt B.O.C, but exist a subset C, add xi into C, will improve B.O.C.
	
	otherwise irrelevant.
	
	Userfullness
	Xi: Given the model/learning algorithm, if xi can help to minimize the error. 
	
14. feature transformation
	Add Hoc Problem. Information Retrieve.

	PCA, ICA
	PCA: for those follow Gaussian Distribution, we can use PCA to pick up the important feature.
	PCA: correlationship ~~ maximize variance ~ reconstruction
	ICA: Independence ~~ ensure mutual information is 0 	~~ statistically independent
		x1,x2 == > y1,y2 ==> I(y1,y2) = 0 and I(y1,x1) as high as possible, I stands for the infomation relationship
		Cocktail Party Problem == voice split
	Blind Source Seperate Problem: ICA
	
	RCA: Random Component Analysis(Random Projection), choose random direction and project the data onto. N dimension Project to M dimension, M << N. to solve curse of dimension. Speed up.
	
	LDA: Lenear Descriminant Analysis, find a projection descriminant with the label. how the data is used in classification.
	     enable data to be projected to a lower space and the points of the same class stay as closer as possible.
	
	Latent Dirichlet Allocation: 
	LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。[1] 
	LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。
	
15. Info Theory
	Joint Entropy: the randomness contained in two variables together,  H(x,y) = - SUM ( P(x,y) log P(x,y) )	
	Conditional entropy: the randomness of one variable given another, H(y|x) = -SUM(P(x,y) log P(y|x)
	Entropy: H(A) = -SUM(P(A) * log(P(A)))
	
	If X || Y is dependent,
	H(y|X) = H(Y)
	H(X,Y) = H(X) + H(Y)
	
	Mutual Infomation
	H(y|x) = I(x,y) = H(y) - H(x|y) = 0 if x, y dependent
	
	KL Divergence(relative entropy): means the difference of two distribution, the distance between two distribution
	D(P||Q) = SUM(P(X) * log ( P(X) / D(X) ) for discrete distribution

	---------reinforced learning--agent reinforced learning
16. Markov Decision Process. MDP,POMDP,HMM

	Markovian Property: 1.only the present state maters for the next state
						2. Stationary: The rules don't change.

	S State, S (including the absorbing state or terminate state)
	A Action, A
	T Transition Model: P(S' | S,A) Given a state and an action, the posibility to next state
	R Rewards:, define the usefulness of into a state.
		R(S): enterring into a state
		R(S,A): on the state and take the action
		R(S,A,S'): On the state, take the action and End in another state.
	
	Policy: a solution of MDP problem.
	   PI(S) ==> A : a function take a state, gives you an action.
	   PI* : the policy that meet the your long term reward. maximize the reward.
	   PI(S,t): time is also important.
	   PI* = argmax E[ SUM(gama^t*R(St)|PI)], t from 0 to infinite.
	   U^pi(S) = E[ SUM(gama^t*R(St)|PI,S0=S)], t from 0 to infinite, long term reward
	           
	   
	   finding an optima policy.
	   
	Credit Assignment Problem. 
	1. delayed reward
	2. small change matters
   
   R(S) = - 0.04 reward: a small minus reward encourge you to end the game.
   R(S) = -2, fiercely encourge you to end the game. HOT BEACH.
   R(S) = 2, encourge you not to end the game. WARM BEACH.
   
   Stationary: infinite horizons
   Stationary Preference: utility(Value Function: tell you long term value from moment to moment) of sequence, sequence of rewards adding up 
   U(s0,s1,s3,...) = SUM(R(s0,s1,s3,..), time to infinite
				==> SUM( gama^t * R(St)), gama between 0<= gama < 1 
				<= SUM(Gama^t * Rmax), Geometric series, = Rmax/(1 - Gama)
				
	*Bellman Equation: the true utility of a state equals the reward being the state + gama * the sum of for all other state, the posibility of from current to state to that state * the utility of that state 
	Bellman Value Function:
	U(S) = R(S) + gama * MAX( SUM(T(S,A,S') * U(S'))), R(S) is the reward of being in S
														   Gama is the number between 0 and 1
														   T(S,A,S') is the posibility from state S to state S' with action A
														   U(S') is the true utility of state(S')
														   MAX is the max value of all directions
														   U is PI*
	Set initial U value for all state. 
	Value Iteration Algorithm(Policy iteration):
	1. start with the arbinary state
	2. update utility based on neighbour(the state it can reach): update all state based on the estimate utility
	3. repeat until all finished(repeatly to add truth to wrong, then wrong is discounted)
	
	*Hanmilton Equation
18. Reinforce learning with Markov Decision Process.
	
	TD prediction:
	
	Q Learning Algorithm:
    Q Function(new value function)
	Q(S,A) = R(S) + gama * SUM(T(S',A)(for all S') * MAX(Q(S',A')) (for all A')
	:value for arriving in S, leaving via A,proceeding optimally thereafter
	Q Value in the max action of all reachable state, The high BellManValue is the one Q Value prefer.
	
	ε-greedy: have eposilon posibility to choose not that good Action during training to provent you get local opima
	Or Randomly restart.
	
	Exploration - Exploitation: something like Simulated annualing, e-greedy. balance that to find the global opitma.
	
	Sarsa Algorithm:
    
19. Game Theory
	Multi-agent conflict, find a solution.
	2-Player Zero-Sum finite deterministic game of perfect information(the reward of 2 player are constant)
	
	I have shamelessly stolen all of Andrew Moore
	
	1. Minimax=== Maximin there exist an optimal (pure strategy) for each player.
	2. Von Neumann: 1 still hold when NON-deterministic game. (2-Player Zero-Sum finite NON-deterministic game of perfect information)
	3. if there exists hidden infomation, then minimax =\== maximin doesn't hold any more. (Mixed Stategy)
	
	Prisoner's Dilemma(囚徒困境): both defeat is remarkablely better choice.
	
	Nash Equilibrium(John Nash): For N players, each of them , no reason for anyone of them to change his strategy( they all find the best strategy for them)
	
	N.E repeat the result will not change.
20. Unknow End, each round, we have gama posibility to continue;
	then the number of rounds is 1/ (1 - gama)
	IPD Strategy:(Tit-For-Tat	针锋相对)
	1. Coperate on the first round
	2. copy the opponent's previous move thereafter
	
	Repeatable games and The Folk Theorem
	:in repeated games, the posibility of relationship opens the door for operation.
	:Describe the set of payoffs that can result from Nash Strategy in repeated games.
	
	Convex hull
	Folk Theorem: repeated game
	
	Pavlv Strategy: Cooperate if agree, defect if disagree(agree mean the two take the same action)(if you attack me, i will attack you)
	
	Computational Folk Theorem
	2 player repeated game, each have its payoff matrix
	Can build Pavlov-like machine for any game. Construct subgame perfect Nash Equilibrium for any game in polynomial time.
	-Pavlov if possible
	-Zero-Sum like 
	-at least one player improve
	
	stochastic game(Markov Game)
	Multi-agent RL
	Zero-Sum multi-agent game: MINIMAX Q Learning
	
	General Sum multi-agent game:Nash Q Learning
	
Unbalanced Data Handling

1. Generated More data in rare class
   1. Directly copy those rare class points and make replication
   2. Add a small eposilon to features of rare class points
   3. SMOTE: choose K(K is replication numbers) nearest neighbours of rare class, and for point x and genterate new points with Xnew = x + rand(0,1) * (XNear - x), XNear is one of the K-Means
   4. Borderline SMOTE: Choose K nearest neighbear for each rare class point x, if x's neighbours most from different class, Then the point is NOISE
																			    if x's neighbours all from the rare class itself, Then the point is SAFE
																				if x's neighbours about half from the rare class, then then point is DANGER
	  Use the danger to generate the new neighbours. It is on the border
	  
	  Borderline-1 SMOTE: the danger point will belongs to the rare class
	  Borderline-2 SMOTE: the danger point will belongs to the ANY classes
	  SVM SMOTE: Use SVM Kernel to define the border and generate new samples( all use the parameter m_neighbors to decide if a sample is in danger, safe, or noise)
	  
	
	
ETF: buy / sell like stock, a group of stocks
Mutual Fund: closed quarterly, less transparent
Hedge Fund: 2,20%, open close by agreement

Stock Value
Intrinsic Value: how much money I will give in future
Book Value: sum of pure assets - intangible asset - liability (or do not count in the intangible asset when calculate assets)
Market Cap: shares * prices

PV: present value
FV: future value

PV = FV / (1+IR)^i : i is how far in the future, IR is intrinsic rate

US Gov IR = 1% for example

Balch Bond IR = 5%

FV: somehow dividend every year for per dollar 

IR ==> discount rate= 0.04 how much you trust the company, if it is 0.04 , that means you will use 0.96 dollar if the company promise you a dollar in a year  

CAMP： Capital Asset Pricing Model 

The market portolio
US: SP500
UK: FTA
Japan: TOPIX 

for a stock i

Returni(t) = betai*returnmarket(t) + alphai(t) ==> E = 0 expected, active manager think they can predict alpha

alpha and beta is calculated through daily return VS SP500 
.alpha is the predict return of the stock
.beta is how much move compared to SP500 up and down

for a portforlio p
returnp(t) = SUM( wi *( betai*returnmarket(t) + alphai(t) )) 
           => betap * returnmarket(t)  + alphap(t)  CAPM
		   => betap * returnmarket(t)  + SUM(alphai(t))  active managers

upward market: large beta is better, earn more
downward market: small beta or even negative beta, loss less
however ===> 
CAMP says you can't beat the market
Efficient Market Hypothesis (EMP) says you can't predict the market		   

   
APT: Arbitrage Pricing Theory
.stephen Ross 1976
returni = betai * returnmarket + alphai
ri = betaiF*returnfinance + betaT*returntechnology + alphai(t)


Use CAMP to remove the market risk through : SUM(wi*betai) = 0
   
selection stocks....
Fundamental Analysis
Technical Analysis

Technical Indicator
.Momentum：price[t] / price[t-n]  - 1
.SMA simple moving average N days moving average:price[t] / price[t-n:t].mean() - 1
.BB Bollinger Band : (price[t] - SMA[t]) / 2 * STD[t]

Normalization and through it into machine learning classification machine


EMH Assumption
Efficient Market hypothesis
Fama in 1960s
.large market investors
.new information arrives randomly
.price adjust the information quickly
.price reflect all available information

3 Forms of EMH 
.Weak: future price can't be predicted by analyzing the history data. =>Wrong
.semi-strong: price adjust to public information quickly. =>Wrong
.Strong: price reflect all information in public and private. =>Wrong

EMH is not correct in some case: the market take time to do adjust

Performance = skill . the SQuare of breath
IR : information ratio

IR = IC * Square of BR(breath)
IC: information coefficient


Don't pick future data in stock training.

Root Mean Square Error = RMSE = Root of (SUM(ytest - ypredict)^2 / n)
Correlation == > x= yPredict, y=Ytest, np.correlation() (-1,1) ==> 1 is great, -1 inverse related, 0 no relation

Ensembling
Bagging
Boosting

Q-Learning

Daily Return as Rewards, ==> immediately return
cumulative return as rewards ==> delayed return

Discretization or Discretizing
K threshold, split all the data in k folders
1. order
2. calculate threshold
3. get spare number with threshold

Dyna-Q for finance 





